---
title: | 
    | **Model Creation: Social Vulnerability Index**
    | **Training Random Forest and Geographically Weighted Random Forest Models**
subtitle: |
    | Thesis for a Master of Public Health, Epidemiology
author: |
    | Nathan Garcia-Diaz
    |
    | Brown University, School of Public Health
date: |
    |
    | `r format(Sys.Date(), '%B %d, %Y')`
mainfont: Times New Roman
fontsize: 11 pt
output:
  pdf_document:
    highlight: tango
  latex_engine: luatex
include-before:
- '`\newpage{}`{=latex}'
---

# Statement of Purpose

The purpose of the file is to build two final models: a traditional random forest model (RF) and a geographically weighted random forest model (GWFRF). The following two sentence provide a overarching description of the two models. In a RF model, each tree in the forest is built from a different bootstrap sample of the training data, and at each node, a random subset of predictors (features) is considered for splitting, rather than the full set of predictors. A GWRF model expands on this concept by incorporating spatial information by weighting the training samples based on their geographic proximity to the prediction location. The splitting process in a RF model is determined by the mean squared error and in a GWRF is influenced by the spatial weights (i.e., weighted mean squared error), which adjust the contribution of each sample based on its geographic distance.

## Overview of Hyperparameters Definitions

In [James et al 2021, Ch 8.2.2 Random Forests](https://www.statlearning.com/), [James et al 2023, Ch 15.2 Definition of Random Forests](https://hastie.su.domains/Papers/ESLII.pdf) and [Garson 2021, Ch 5 Random Forest](https://www.amazon.com/Data-Analytics-Social-Sciences-Applications/dp/0367624273), the others highlight shared parameters between the RF and GWRF models:

-   **Number of randomly selected predictors**: This is the number of predictors (p) considered for splitting at each node. It controls the diversity among the trees. A smaller m leads to greater diversity, while a larger m can make the trees more similar to each other.
    -   for regression this defaults to $p/3$, where *p* is the total of predictor variables
-   **Number of trees**: This is the total number of decision trees in the forest (m). More trees generally lead to a more stable and accurate model, but at the cost of increased computational resources and time.
    -   for the `randomForest::randomForest()`, this defaults to 500

Additionally, GWRF involves an extra tuning spatial parameters:

-   **Bandwidth parameter**: This controls the influence of spatial weights, determining how quickly the weight decreases with distance. A smaller bandwidth means only very close samples have significant influence, while a larger bandwidth allows more distant samples to also contribute to the model.

## Outline of Hyper-parameter Tuning Process

4 RF models will be built, and they differ based on the different hyperparameters: (1) default settings, (2) first tune *p*, and subsequently tune, then *m* while keeping *p* constant, (3) simultaneously tune *m* and *p* with a grid search, (4) tuned with Out of Bag MSE Error Rates as described by Garson 2021. Two metrics will be implemented in the tuning process: Root Mean Squared Error and Out of Bag Error Rate.

In Garson 2021, Ch 5 Random Forest, Garson teaches Random Forest Models by using `randomForest::randomForest()`, and in chapter 5.5.9 (pg. 267), he provides methods for tuning both of these parameters simultaneously using the Out of Bag MSE Error Rates. This value is a measure of the prediction error for data points that were not used in training each tree, and it can be written as $\text{OOB Error Rate} = \frac{1}{n} \Sigma^{N}_{i=1} (y_i - \hat y_i^{\text{OOB}})^2$ . $\hat y_i^{\text{OOB}}$ is the OOB prediction for the i-th observation, which is obtained by averaging the predictions from only those trees that did not include i in their bootstrap sample. To provide a high-level summary, since each tree in a Random Forest is trained on a bootstrap sample (a random sample with replacement) of the data, approximately one-third of the data is not used for training each tree. This subset of data is referred to as the "out-of-bag" data for that tree, and this value is calculated using the data points that were not included in the bootstrap sample used to build each tree.

[Georganos et al (2019)](https://www.tandfonline.com/doi/full/10.1080/10106049.2019.1595177) created the `package(SpatialML)`, and subsequently the tuning is made possible by the `SpatialML::grf.bw()` function. The function uses an exhaustive approach (i.e., it tests sequential nearest neighbor bandwidths within a range and with a user defined step, and returns a list of goodness of fit statistics).

4 RF models will be built, and they differ based on the different hyperparameters: (1) default settings; (2) tuned by first tuning *mtry*, with *ntrees* set to default, and subsequently tuning then *ntrees* while keeping the newly defined *mtry* constant and both methods use RMSE as the metric; (3) tuned both *mtry* and *ntrees* with an Exhaustive Grid Search and both methods use RMSE as the metric, (4) tune tuned with Out of Bag MSE Error Rates as described by Garson 2021. For each model, MAE, MSE, RMSE, and $R^2$ will be calculated and the hyperparameters of the best model will continue onto the GWRF. To provide points of comparison in the GWRF, two additional models will be created. Thus, three GWRF models will be created: (1) default *mtry* and *ntrees* with optimized *bandwidth parameter*, (2) using the previously defined best hyperparameters, (3) using the optimized *bandwidth parameter* in step one, then tuning *mtry*, with *ntrees* set to default. The method for GWRF Model 3 uses Out of Bag Error Rate as the Metric. The same model evaluation metrics will be compared in addition to calculating the residual autocorrelation.

Lastly, the feature importance plots will be generated for the final, and local feature importance plots will also be created.

\newpage

# Preparation

```{r, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(fig.width=6, fig.height=4) 
options(tigris_use_cache = TRUE)
options(repos = c(CRAN = "https://cran.r-project.org"))
```

```{r preparation}
### importing packages
# define desired packages 
library(tidyverse)    # general data manipulation
library(knitr)        # Rmarkdown interactions
library(here)         # define top level of project folder
                          # this allows for specification of where 
                          # things live in relation to the top level
library(foreach)      # parallel execution
# spatial tasks
library(tigris)       # obtain shp files 
library(spdep)        # exploratory spatial data analysis
# random forest 
library(caret)        # machine learning model training 
library(rsample)      # splitting testing/training data
library(randomForest) # traditional RF model
library(SpatialML)    # spatial RF model
# others 
library(foreach)      # parrallel processing
library(ggpubr)       # arrange multiple graphs

### setting seed
set.seed(926) 

### loading data 
svi_df = read_csv(here::here("01_Data", "svi_df.csv")) %>% 
  mutate(fips = as.character(fips)) %>% 
  select(-...1)

### obtaining SPH files for RI tracts
tracts = tracts(state = "RI", year = 2022, cb = TRUE)

### joining data 
svi_df = inner_join(tracts, svi_df, by = c("GEOID" = "fips"))

### defining analytical coordinates and df 
df_coords = svi_df %>% 
  mutate(
    # redefines geometry to be the centroid of the polygon
    geometry = st_centroid(geometry),
    # pulls the lon and lat for the centroid
    lon = map_dbl(geometry, ~st_point_on_surface(.x)[[1]]),
    lat = map_dbl(geometry, ~st_point_on_surface(.x)[[2]])) %>% 
  # removes geometry, coerce to data.frame
  st_drop_geometry() %>%
  # only select the lon and lat 
  select(lon, lat)

# only obtain response and predictor variables 
df = svi_df %>% 
  st_drop_geometry() %>% 
  select(rpl_themes, starts_with("e_")) 
```

# Traditional Random Forest Model

## Model Training and Hyperparameter Tuning

Models will be created and compared at the end of the section.

### RF Model 1 - Default Settings

**Background**: The default settings for the RF model is *mtry* = p/3, and ntrees = 500, where *p* is the number of predictors.

```{r rf_mod1}
### setting seed
set.seed(926)

# obtain the number of predictors
pred_num = svi_df %>% 
  st_drop_geometry() %>% 
  select(starts_with("e_")) %>% 
  colnames() %>% 
  length()
# determine the default number of predictors
mtry = round(pred_num / 3)

# creating the first model
rf_mod1 = train(rpl_themes ~., 
                 data = df, 
                 method = "rf", 
                 trControl = trainControl(method = "cv", number = 10), 
                 tuneGrid = expand.grid(mtry = mtry), 
                 ntree = 500, 
                 importance = TRUE)

# Print the results
rf_mod1$finalModel
```

### RF Model 2 - Sequential Processing With RMSE Metric

**Background**: This model training process uses a combination of sequential processing and cross-validation. First, tuning the `mtry` parameter by using cross-validation to find the best value for each iteration. The model runs 10 times (i.e., the for loop) because given the nature of the building random forest models, the value of m within the loop changes. Therefore, preforming the function 10 times and taking the average of the most optimal mtry value it calculates and prints the average of the best mtry values. During the second step, the ntree is changing and cross-validated while *mtry* is held constant.

```{r rf_mod2}
### setting seed
set.seed(926)

### Step 1: Find the best `mtry` value
# Create an empty list to store the results
results_list = vector("list", 10)
# Loop to repeat the code 10 times
for (i in 1:10) {
  # Train the random forest model with 10-fold cross-validation
  rf_mod2 = train(rpl_themes ~ ., data = df, method = "rf", 
                  ntree = 500, # Start with a default number of trees
                  trControl = trainControl(method = "cv", number = 10),
                  tuneGrid = expand.grid(mtry = c(3:8)))
  
  # print model results 
  # print(rf_mod2)
  plot(rf_mod2)
  
  # Extract the best number of predictors (mtry) from the model
  m = rf_mod2$bestTune$mtry
  
  # Store the result in the list
  results_list[[i]] = m
}

mean_mtry = round(mean(unlist(results_list)))

# Step 2: Find the best `ntree` value using cross-validation with the optimal `mtry`
store_maxtrees = list()
ntree_values = c(250, 300, 350, 400, 450, 500, 550, 600, 800, 1000) # List of `ntree` values to test

for (ntree in ntree_values) {
    rf_maxtrees = train(rpl_themes ~ ., data = df, method = "rf",
                        tuneGrid = expand.grid(mtry = mean_mtry), # Use the fixed best `mtry`
                        trControl = trainControl(method = "cv", number = 10),
                        ntree = ntree)
    # print model results 
    # print(rf_maxtrees)

    store_maxtrees[[as.character(ntree)]] <- rf_maxtrees
}

# 500 subjectively made 
# summary(resamples(store_maxtrees))

# creating the first model
rf_mod2 = train(rpl_themes ~., 
                 data = df, 
                 method = "rf", 
                 trControl = trainControl(method = "cv", number = 10), 
                 tuneGrid = expand.grid(mtry = mean_mtry), 
                 ntree = 300, 
                 importance = TRUE)

# Print the results
rf_mod2$finalModel
```

```{r}
# Initialize an empty data frame to store all metrics
metrics_df <- data.frame()

# Loop through each model in store_maxtrees
for (ntree in names(store_maxtrees)) {
  model <- store_maxtrees[[ntree]]
  
  # Extract metrics for the current model
  model_metrics <- data.frame(
    ntree = as.numeric(ntree),                  # Capture the ntree value
    RMSE = model$results$RMSE,                  # Extract RMSE
    MAE = model$results$MAE,                    # Extract MAE
    Rsquared = model$results$Rsquared,          # Extract Rsquared
    mtry = model$bestTune$mtry                  # Capture the mtry value
  )
  
  # Append the metrics to the main data frame
  metrics_df <- rbind(metrics_df, model_metrics)
}


# Create the line graph
a=ggplot(metrics_df, aes(x = ntree, y = RMSE)) +
  geom_point(size = 2) +
  labs(title = "RMSE Across Different ntree",
       x = "Number of Trees (ntree)",
       y = "Root Mean Squared Error",
       shape = "ntree") +
  theme_bw()

b=ggplot(metrics_df, aes(x = ntree, y = MAE)) +
  geom_point(size = 2) +
  labs(title = "MAE Across Different ntree",
       x = "Number of Trees (ntree)",
       y = "Mean Absolute Error",
       shape = "ntree") +
  theme_bw()

c=ggplot(metrics_df, aes(x = ntree, y = Rsquared)) +
  geom_point(size = 2) +
  labs(title = "Rsquared Across Different ntree",
       x = "Number of Trees",
       y = "R-squared",
       shape = "ntree") +
  theme_bw()


blank = ggplot() + theme_void()

fig = ggarrange(
    ggarrange(a, b, blank, nrow = 1,
        widths = c(1, 1, 0), legend = "none"),
    ggarrange(blank, c, blank, nrow = 1,
        widths = c(0.5, 1, 0.5), legend = "none"),
    nrow = 2,
    legend = "none"
)
annotate_figure(fig, bottom = text_grob("mtry = 6", hjust = 1, x = 1))
```

The second model hyperparameters have been set to *mtry* = `r rf_mod2$bestTune`, and *ntrees* = `r rf_mod2$finalModel$ntree`. 

### Model 3 - Exhaustive Grid Search with RMSE as Metric

**Background**: To preform an exhaustive Grid Search, [Brownlee (2020)](https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/) created a custom function that preforms the grid search. This function checks every combination of *mtry* and *ntree* values determines the final values with RMSE.

```{r}
### setting seed
set.seed(926)

# Define the tuned parameter
grid = expand.grid(.mtry = c(3:8), 
                   .ntree = c(250, 300, 350, 400, 450, 500, 550, 600, 800, 1000) )

ctrl = trainControl(method = "cv", number = 10)

# create custom 
customRF <- list(type = "Regression", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes

# creating the first model
rf_mod3 = train(rpl_themes ~., 
                 data = df, 
                 method = customRF, 
                 trControl = ctrl, 
                 tuneGrid = grid, 
                 importance = TRUE)

# Print the results
rf_mod3$finalModel
```

```{r}
plot(rf_mod3)

metrics_df = as.data.frame(rf_mod3$results)

# Create the line graph
ggplot(metrics_df, aes(x = ntree, y = RMSE, color = as.factor(mtry))) +
  geom_line() +
  geom_point(size = 2) +
  labs(title = "RMSE Across Different ntree",
       x = "Number of Trees (ntree)",
       y = "Root Mean Squared Error",
       shape = "ntree") +
  theme_bw()

ggplot(metrics_df, aes(x = ntree, y = MAE, color = as.factor(mtry))) +
  geom_line() +
  geom_point(size = 2) +
  labs(title = "MAE Across Different ntree",
       x = "Number of Trees (ntree)",
       y = "Mean Absolute Error",
       shape = "ntree") +
  theme_bw()

ggplot(metrics_df, aes(x = ntree, y = Rsquared, color = as.factor(mtry))) +
  geom_line() +
  geom_point(size = 2) +
  labs(title = "Rsquared Across Different ntree",
       x = "Number of Trees (ntree)",
       y = "R-squared",
       shape = "ntree") +
  theme_bw()
```

The final values used for the model were *mtry* = `r rf_mod3$finalModel$mtry` and *ntree* = `r rf_mod3$finalModel$ntree`. Note that the variation between each of the combinations is minimal.

### Model 4

This code snippet is designed to optimize the hyperparameters mtry and ntree in a Random Forest model and by examining the OOB MSE across these combinations, the code identifies which parameters yield the lowest error, helping to optimize the Random Forest model. Here's how the code meets this objective:

-   Iterative Search for mtry: The mtry_iter function generates an iterable sequence of mtry values, starting from 1 up to the number of predictors, incremented by a step factor. This allows the code to explore different numbers of predictors used at each split in the trees.
-   Specification of ntree Values: A predefined vector vntree contains different values for the number of trees to be grown in the forest. This allows the code to assess how the number of trees impacts the model performance.
-   Error Calculation Across Hyperparameter Combinations: The tune function performs a grid search over the specified mtry values and the maximum number of trees specified in vntree. For each combination, the function trains a Random Forest model and calculates the OOB error rate (MSE if y is continuous).
-   Parallel Processing: The foreach loop with the .dopar argument allows for parallel execution of the grid search, which speeds up the computation.
-   Result Aggregation: The results are combined into a data frame, which can then be analyzed to identify the optimal combination of mtry and ntree that minimizes the OOB error rate.

This approach ensures that both hyperparameters are tuned simultaneously, leading to a more efficient model optimization process. The final model hyperparameters have been set to *m* = 9, and *ntrees* = 501. The graph below illustrates that the errors across the hyperparameters used with this method are very similar.

```{r modified Garson Method}
# create an interaction function to search over different values of mtry
mtry_iter = function(from, to, stepFactor = 1.05){
  nextEl = function(){
    if (from > to) stop('StopIteration')
    i = from 
    from <<- ceiling(from * stepFactor)
    i
  }
  obj = list(nextElem = nextEl)
  class(obj) = c('abstractiter', 'iter')
  obj
}

# create a vector of ntree values of interest
vntree = c(51, 101, 501, 1001, 1501)

# specify the predictor (x) and outcome (y) object
x = df %>% select(starts_with("e_")) %>% st_drop_geometry()
y = df %>% pull(rpl_themes)

# Create a function to get random forest error information for different mtry values 
tune = function(x, y, ntree = vntree, mtry = NULL, keep.forest = FALSE, ...) {
  
  # Define the combination function to aggregate results
  comb = function(a, b) {
    if (is.null(a)) return(b)
    rbind(a, b)
  }
  results = foreach(mtry = mtry_iter(1, ncol(x)), .combine = comb, .packages = 'randomForest') %dopar% {
    model = randomForest::randomForest(x, y, ntree = max(ntree), mtry = mtry, keep.forest = FALSE)
    if (is.factor(y)) {
      errors = data.frame(ntree = ntree, mtry = mtry, error = model$err.rate[ntree, 1])
    } else {
      errors = data.frame(ntree = ntree, mtry = mtry, error = model$mse[ntree])
    }
    return(errors)
  }
  return(results)
}

# running the tuning 
results = tune(x,y) %>% 
  mutate(MSE = error) %>% 
  select(-error)

# examinations of other hyperparameters
# table
temp = results %>% 
  arrange(MSE) %>% 
  head() 

kable(temp, caption = "Model 4 Preformance Metrics", digits = 4, align = c("l", "l", "c"))

# plot
ggplot(results, aes(y = MSE, x = ntree, 
                    color = as.factor(mtry))) + 
  geom_point() + 
  geom_line() + 
  theme_bw() +
  labs(color = "mtry", y = "MSE Error Rate") 

best_mtry = temp$mtry[[1]]
best_ntree = temp$ntree[[1]]

# creating the first model
rf_mod4 = train(rpl_themes ~., 
                 data = df, 
                 method = "rf", 
                 trControl = trainControl(method = "cv", number = 10), 
                 tuneGrid = expand.grid(mtry = best_mtry), 
                 ntree = best_ntree, 
                 importance = TRUE)

# Print the results
rf_mod4$finalModel
```

The final model hyperparameters have been set to *mtry* = `r best_mtry`, and *ntrees* = `r best_ntree`. The graph below illustrates that the errors across the hyperparameters used with this method are very similar.

## RF Model Evaluation

Despite variations in the *m* and *ntrees* parameters across different models, the overall prediction performance remains consistent. The relatively low MSE and RMSE values across the models indicate that the predictions are generally close to the actual values. The high R-Squared values suggest that each model explains a significant portion of the variance in the target variable. However, since model 4 produced code that is lowest MSE and RMSE, and highest R-squared value, these are the parameters that will be head contains for the GWRF.

-   Mean Absolute Error (MAE):$\frac{1}{n}\Sigma^n_{i=1}|y_i - \hat y_i |$
-   Mean Squared Error (MSE): $\frac{1}{n}\Sigma^n_{i=1}(y_i - \hat y_i)^2$
-   Root Mean Squared Error (RMSE):$\sqrt{\frac{1}{n}\Sigma^n_{i=1}(y_i - \hat y_i)^2}$
-   R-Squared Value: $\frac{\Sigma(y - \hat y )^2}{\Sigma (y - \bar y)^2}$

```{r}
# Create a data frame with the results
results_rf = data.frame(
  Model = c("Model 1", "Model 2", "Model 3", "Model 4"),
  mtry = c(rf_mod1$results$mtry,
           rf_mod2$results$mtry,
           rf_mod3$bestTune[[1]],
           rf_mod4$results$mtry),
  ntree = c(500,550,540,501),
  MAE = c(rf_mod1$results$MAE,
           rf_mod2$results$MAE,
           mean(rf_mod3$results$MAE),
           rf_mod4$results$MAE),
  RMSE = c(rf_mod1$results$RMSE,
           rf_mod2$results$RMSE,
           mean(rf_mod3$results$RMSE),
           rf_mod4$results$RMSE),
  R_Squared = c(rf_mod1$results$Rsquared,
           rf_mod2$results$Rsquared,
           mean(rf_mod3$results$Rsquared),
           rf_mod4$results$Rsquared))

# Print the results using kable
kable(results_rf, caption = "Performance Metrics for Each Model", 
      digits = 3, align = c("l", "c", "c", "c", "c", "c", "c"))
```

# Training a Geographically Weighted Random Forest Model

## GWRF Model 1

This model has hyperparameters defined with mtry and trees by the default: *bandwith* = 49, trees = 500 and mtry = 5.

```{r gwrf mod 1}
# testing for optimal bandwidth
temp = SpatialML::grf.bw(rpl_themes ~ e_pov150 + e_unemp + e_hburd + e_nohsdp + 
                               e_uninsur + e_age65 + e_age17 + e_disabl + 
                               e_sngpnt + e_limeng + e_minrty + e_munit +
                               e_mobile + e_crowd + e_noveh + e_groupq, 
                             dataset = df, 
                             kernel = "adaptive",
                             bw.min = 20,
                             bw.max = 50,
                             coords = df_coords,
                             trees = 500, 
                             mtry = mtry, 
                             step = 1, importance = "impurity")
best.bw_gwrf_mod1 = temp$Best.BW

# defining the spatial model with prior model hyparameters
gwrf_mod1 = SpatialML::grf(rpl_themes ~ e_pov150 + e_unemp + e_hburd + e_nohsdp + 
                           e_uninsur + e_age65 + e_age17 + e_disabl + 
                           e_sngpnt + e_limeng + e_minrty + e_munit +
                           e_mobile + e_crowd + e_noveh + e_groupq,
                         dframe = df, 
                         kernel = "adaptive",
                         coords = df_coords,
                         bw = best.bw_gwrf_mod1,
                         ntree = 500, 
                         mtry = mtry, 
                         importance = "impurity")

```

The final model hyperparameters have been set to *bandwidth* = `r best.bw_gwrf_mod1`, *mtry* = `r mtry`, and *ntrees* = 500.

## GWRF Model 2

This model contains the hyperparameters defined in the RF building section: *bandwidth* = 44, *trees* = 501 and *mtry* = 15.

```{r gwrf mod 2}
# testing for optimal bandwidth
temp = SpatialML::grf.bw(rpl_themes ~ e_pov150 + e_unemp + e_hburd + e_nohsdp + 
                               e_uninsur + e_age65 + e_age17 + e_disabl + 
                               e_sngpnt + e_limeng + e_minrty + e_munit +
                               e_mobile + e_crowd + e_noveh + e_groupq, 
                             dataset = df, 
                             kernel = "adaptive",
                             bw.min = 20,
                             bw.max = 50,
                             coords = df_coords,
                             trees = best_ntree, 
                             mtry = rf_mod4$results$mtry, 
                             step = 1)

best.bw_gwrf_mod2 = temp$Best.BW

# defining the spatial model with prior model hyparameters
gwrf_mod2 = SpatialML::grf(rpl_themes ~ e_pov150 + e_unemp + e_hburd + e_nohsdp + 
                           e_uninsur + e_age65 + e_age17 + e_disabl + 
                           e_sngpnt + e_limeng + e_minrty + e_munit +
                           e_mobile + e_crowd + e_noveh + e_groupq,
                         dframe = df, 
                         kernel = "adaptive", 
                         coords = df_coords,
                         bw = best.bw_gwrf_mod2,
                         ntree = best_ntree, 
                         mtry = rf_mod4$results$mtry, 
                         importance.mode = "impurity") # this is a ranger argument
                                                       # specification of this value 
                                                       # corrected errors that previously appeared
                                                       # no importance value specified
```


The final model hyperparameters have been set to *bandwidth* = `r best.bw_gwrf_mod2`, *mtry* = `r rf_mod4$results$mtry`, and *ntrees* = `r best_ntree`.

## GWRF Model Evaluation

The models both preform nearly identically because the hyperparameters preform nearly identically. Therefore, the model define by the previous traditional random forest model.

```{r}
# Model 5
predictions5 = gwrf_mod1$Global.Model$predictions
mse5 = mean((df$rpl_themes - predictions5)^2)
rmse5 = sqrt(mse5)
mae5 = sum(abs(df$rpl_themes - predictions5))/length(predictions5)
r_squared5 = 1 - sum((df$rpl_themes - predictions5)^2) / sum((df$rpl_themes - mean(df$rpl_themes))^2)

# Model 6
predictions6 = gwrf_mod2$Global.Model$predictions
mse6 = mean((df$rpl_themes - predictions6)^2)
rmse6 = sqrt(mse6)
mae6 = sum(abs(df$rpl_themes - predictions6))/length(predictions6)
r_squared6 = 1 - sum((df$rpl_themes - predictions6)^2) / sum((df$rpl_themes - mean(df$rpl_themes))^2)

# Create a data frame with the results
results_grf = data.frame(
  Model = c("Model 5", "Model 6"),
  bw = c(best.bw_gwrf_mod1, best.bw_gwrf_mod2),
  mtry = c(5, 9),
  ntree = c(500,501),
  MAE = c(mae5, mae6),
  RMSE = c(rmse5, rmse6),
  R_Squared = c(r_squared5, r_squared6)
)

# Print the results using kable
kable(results_grf, caption = "Performance Metrics for Each Model", 
      digits = 3, align = c("l", "c", "c", "c", "c", "c", "c"))
```

# Random Forest Model Comparisons

Model 4 shows a lower MAE and MSE, indicating that it generally makes smaller errors in prediction. The RMSE is also relatively low, and the high $R^2$ value (0.791) suggests that this model explains a significant portion of the variance in the SVI. Overall, Model 4 performs well and is effective in predicting SVI using the selected predictors.

Model 6 has higher MAE and MSE values, indicating that it makes larger errors on average compared to Model 4. The RMSE is also higher, and the $R^2$ is lower (0.638), suggesting that Model 6 explains less variance in the SVI. This could mean that while the Geographically Weighted Random Forest accounts for spatial autocorrelation, it may not perform as well in terms of overall prediction accuracy as the traditional Random Forest.

Model 4 (Traditional Random Forest) outperforms Model 6 (Geographically Weighted Random Forest) in terms of accuracy and explained variance. However, Model 6 is still valuable because it accounts for spatial dependencies. Model 6 might be more appropriate despite its lower overall performance metrics, particularly if the goal is to understand regional variations in the SVI. However, if the focus is purely on predictive accuracy, Model 4 appears to be the better choice.

```{r}
final_results = results_rf[4,] %>% 
  mutate(bw = "-") %>% 
  select(Model, bw, mtry, ntree, MAE, RMSE, R_Squared)

final_results = rbind(final_results, results_grf[2,]) 

# Print the results using kable
kable(final_results, caption = "Performance Metrics for Each Model", 
      digits = 3, align = c("l", "r","r","r","r","r","r"))
```

## Graphs for the Final Models

### Traditional Random Forest

```{r}
df = df %>% 
  mutate(rf_pred = as.data.frame(rf_mod4$finalModel$predicted)[[1]])

# Predicted 1:1 Plot
ggplot(df, aes(x = rf_pred, y = rpl_themes)) +
  geom_point() + 
  theme_bw() + 
  geom_abline(slope=1, intercept=0,linetype="dashed",size=0.5) + 
  geom_smooth(method = "lm", se = FALSE, colour="black",size=0.5) + 
  labs(x="Observed", y = "Predicted")

# Variable Importance
temp = as.data.frame(rf_mod4$finalModel$importance)
colnames(temp) = c("IncMSE", "IncNodePurity")
temp = tibble::rownames_to_column(temp, "Variable") 

ggplot(temp, aes(x = fct_reorder(Variable, IncMSE), y = IncMSE)) +
  geom_bar(stat = "identity", fill = "grey") +
  coord_flip() +  # Flip coordinates for better readability
  labs(title = "Variable Importance (Increase in %IncMSE)",
       subtitle = "Traditional Random Forest Model",
       x = "Variable",
       y = "Increase in MSE") +
  theme_bw()

# Generate Partial Dependence Plots and store them as ggplot objects
a = as.data.frame(randomForest::partialPlot(rf_mod4$finalModel, df, e_limeng, plot = FALSE))
b = as.data.frame(randomForest::partialPlot(rf_mod4$finalModel, df, e_pov150, plot = FALSE))
c = as.data.frame(randomForest::partialPlot(rf_mod4$finalModel, df, e_noveh, plot = FALSE))
d = as.data.frame(randomForest::partialPlot(rf_mod4$finalModel, df, e_munit, plot = FALSE))

# Convert the base R plots to ggplot objects
plot_a <- ggplot(a, aes(x, y)) + geom_line() + labs(title = "Partial Dependence of e_limeng")
plot_b <- ggplot(b, aes(x, y)) + geom_line() + labs(title = "Partial Dependence of e_pov150")
plot_c <- ggplot(c, aes(x, y)) + geom_line() + labs(title = "Partial Dependence of e_noveh")
plot_d <- ggplot(d, aes(x, y)) + geom_line() + labs(title = "Partial Dependence of e_munit")

# Arrange the plots in a grid
gridExtra::grid.arrange(plot_a, plot_b, plot_c, plot_d, nrow = 2, ncol = 2)
```

### Geographically Weighted Random Forest

I am unsure as to how to obtain local variable importance maps. Currently I have local variable importance for 174 observations or census tracts. This aligns with the number of observations/census tracts in the training data sets, therefore, making a map of the entire state is not possible as I do not have values of for those census tracts in the testing data set.

To remedy this situation, I propose changing the analytically plan such that the model training and testing occurs on two different years of the same data (i.e., train on 2022, and test on 2021). This would allow me to obtain variable importance values across all values of the SVI.

```{r grf predicted results}
# Spatial Distribution of Prediction and Observation Summary Index Values
svi_df = svi_df %>% 
  mutate(grf_pred = gwrf_mod1$Global.Model$predictions)

# Predicted 1:1 Plot
ggplot(svi_df, aes(x = grf_pred, y = rpl_themes)) +
  geom_point() + 
  theme_bw() + 
  geom_abline(slope=1, intercept=0,linetype="dashed",size=0.5) + 
  geom_smooth(method = "lm", se = FALSE, colour="black",size=0.5) + 
  labs(x="Observed", y = "Predicted")

a = ggplot(svi_df, aes(fill = rpl_themes)) +
  geom_sf() + 
  theme_void() +
  scale_fill_viridis_c() + 
  labs(title = "Observed SVI Values") +
  theme(legend.title=element_blank())

b = ggplot(svi_df, aes(fill = grf_pred)) +
  geom_sf() + 
  theme_void() +
  scale_fill_viridis_c() + 
  labs(title = "Predicted SVI Values") + 
  theme(legend.title=element_blank())

ggarrange(a,b, ncol = 2, common.legend = TRUE, legend="bottom")
```

```{r grf importance results}
# Global Variable Importance
temp = as.data.frame(gwrf_mod1$Global.Model$variable.importance)
colnames(temp) = c("IncMSE")
temp = tibble::rownames_to_column(temp, "Variable")
ggplot(temp, aes(x = reorder(Variable, IncMSE), y = IncMSE)) +
  geom_bar(stat = "identity", fill = "grey") +
  coord_flip() +  # Flip coordinates for better readability
  labs(title = "Variable Importance (Increase in %IncMSE)",
       subtitle = "Traditional Random Forest Model",
       x = "Variable",
       y = "Increase in MSE") +
  theme_bw()

# Local Variable Importance 
variable_names = colnames(gwrf_mod1$Local.Variable.Importance)
plot_lst = list()

# Loop through the names and create maps
for (var_name in variable_names) {
  
  # Generate the map using ggplot2
  p <- ggplot(svi_df, aes_string(fill = var_name)) +
    geom_sf() +
    scale_fill_viridis_c() +
    labs(title = paste("Map of Local Variable Importance:", var_name),
         fill = "Importance") +
    theme_void()
  
  # Save the plot or print it
  plot_lst[[var_name]] = p
}

############ SocioEconomic Status
ggarrange(plot_lst[["e_pov150"]], plot_lst[["e_unemp"]])
ggarrange(plot_lst[["e_hburd"]], plot_lst[["e_nohsdp"]])
plot_lst[["e_uninsur"]]
################ Household Characteristics 
ggarrange(plot_lst[["e_age65"]], plot_lst[["e_age17"]])
ggarrange(plot_lst[["e_disabl"]], plot_lst[["e_sngpnt"]])
plot_lst[["e_limeng"]]
################# Racial & Ethnic Minority Status
plot_lst[["e_minrty"]]
################# Housing Type & Transportation 
ggarrange(plot_lst[["e_munit"]], plot_lst[["e_mobile"]])
ggarrange(plot_lst[["e_crowd"]], plot_lst[["e_noveh"]])
plot_lst[["e_groupq"]]
```
